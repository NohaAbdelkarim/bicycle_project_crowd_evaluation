{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Gather insights about the annotators:\n",
    "    a. How many annotators did contribute to the dataset?\n",
    "    b. What are the average, min and max annotation times (durations)? Feel free to\n",
    "        add visual representations here such as graphs if you like.\n",
    "    c. Did all annotators produce the same amount of results, or are there\n",
    "        differences?\n",
    "    d. Are there questions for which annotators highly disagree?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The answer of the question: How many annotators did contribute to the dataset?\n",
    "\n",
    "# Read the results CSV file to analyze the 'vendor_user_id' column\n",
    "results_data = pd.read_csv('results.csv')\n",
    "# Extract the 'vendor_user_id' column\n",
    "vendor_user_id = results_data['vendor_user_id'].tolist()\n",
    "# determine the count of annotator from getting the length of the annotators list without repetition\n",
    "annotators_count = len(set(vendor_user_id))\n",
    "# print the annotators count\n",
    "print(\"The anonymized_project dataset has \"+ str(annotators_count) + \" annotators\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The answer of the question: What are the average, min and max annotation times (durations)? Feel free to\n",
    "#add visual representations here such as graphs if you like.\n",
    "\n",
    "# Extract the 'duration_ms' column\n",
    "duration_ms = results_data['duration_ms'].tolist()\n",
    "# Determine the positive time list as time can not be negative number\n",
    "duration_ms_positive=[i for i in duration_ms if i > 0]\n",
    "# Determine the average annotation time\n",
    "average_annotation_time = sum(duration_ms_positive)/len(duration_ms_positive)\n",
    "# Determine the max annotation time\n",
    "max_annotation_time = max(duration_ms_positive)\n",
    "# Determine the min annotation time\n",
    "min_annotation_time = min(duration_ms_positive)\n",
    "# print the average annotation time\n",
    "print(\"The average annotation time is \" + str(average_annotation_time))\n",
    "# print the max annotation time\n",
    "print(\"The max annotation time is \" + str(max_annotation_time))\n",
    "# print the min annotation time\n",
    "print(\"The min annotation time is \" + str(min_annotation_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The answer of the question: Did all annotators produce the same amount of results, or are there\n",
    "# differences?\n",
    "def annotator_count(vendor_user_id):\n",
    "    annotator_contribution_list=[]\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_01\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_02\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_03\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_04\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_05\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_06\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_07\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_08\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_09\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_10\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_11\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_12\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_13\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_14\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_15\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_16\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_17\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_18\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_19\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_20\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_21\"))\n",
    "    annotator_contribution_list.append(vendor_user_id.count(\"annotator_22\"))\n",
    "    return annotator_contribution_list\n",
    "annotator_contribution_list = annotator_count(vendor_user_id)\n",
    "print('The annotators contribution list: ' + str(annotator_contribution_list))\n",
    "print('As shown in the annotators contribution list, the annotators participate with a different amount of results')\n",
    "plt.plot(annotator_contribution_list,'*')\n",
    "plt.title('The amount of contribution for each annotator')\n",
    "plt.ylabel('contribution')\n",
    "plt.xlabel('annotators')\n",
    "plt.savefig('annotator_contribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The answer of the question: Are there questions for which annotators highly disagree?\n",
    "# Determine the ID of each question in the dataset from the 'anonymized_project.json' file \n",
    "DataSet = [json.loads(line) for line in open('anonymized_project.json', 'r', encoding = 'utf-8')]\n",
    "results=DataSet[0]['results']['root_node']\n",
    "NodeInputID_List = []\n",
    "for NodeInputID in results['results']:\n",
    "    try: \n",
    "            NodeInputID_temp = NodeInputID\n",
    "            NodeInputID_List.append(NodeInputID_temp)\n",
    "                \n",
    "    except:\n",
    "         pass\n",
    "results=DataSet[0]['results']['root_node']['results'][NodeInputID_List[0]]['results']\n",
    "#Determine the number of input nodes in the dataset\n",
    "InputNodesLenght=len(NodeInputID_List)\n",
    "# Determine the number of questions in each input node\n",
    "InputNodeQuestions=len(results)\n",
    "\n",
    "# Extract the 'answer' column\n",
    "answer = results_data['answer'].tolist()\n",
    "answer_array = np.array(answer)\n",
    "shape = ( InputNodesLenght, InputNodeQuestions )\n",
    "# Shape the annotator's answers list into a matrix \n",
    "# to organize the answers of each question in a separate row\n",
    "answer_matrix=answer_array.reshape( shape )\n",
    "similar=[]\n",
    "no_count=0\n",
    "yes_count=0\n",
    "# Scan each row in the matrix to get the amount of similarity in each question answer\n",
    "for rows in range(0, answer_matrix.shape[0]):\n",
    "    for columns in range(0, answer_matrix.shape[1]):\n",
    "        if answer_matrix[rows][columns] =='yes':\n",
    "            yes_count=yes_count+1\n",
    "        elif answer_matrix[rows][columns] =='no':\n",
    "            no_count=no_count+1\n",
    "    number_of_similarities=max([yes_count,no_count])\n",
    "    temp= number_of_similarities/10\n",
    "    similar.append(temp)\n",
    "    no_count=0\n",
    "    yes_count=0\n",
    "\n",
    "# Use the questions ID list to determine the questions for which annotators highly disagree\n",
    "# by using the indices of the questions which have similar answers with only 50%  \n",
    "questions_highly_disagree=[]\n",
    "for index, item in enumerate(similar):\n",
    "    if item == 0.5:\n",
    "        temp=NodeInputID_List[index]\n",
    "        questions_highly_disagree.append(temp)\n",
    "\n",
    "questions_highly_disagree_df = pd.DataFrame(columns=['QuestionsHighlyDisagreeID'])\n",
    "questions_highly_disagree_df.QuestionsHighlyDisagreeID=questions_highly_disagree\n",
    "print(\"There are \"+ str(len(questions_highly_disagree))+ \" questions for which annotators highly disagree which are:\")\n",
    "print(questions_highly_disagree)\n",
    "# Save the determined questions ID in CSV file\n",
    "questions_highly_disagree_df.to_csv('questions_highly_disagree.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Besides picking yes or no the annotators had the chance to tell if the data were\n",
    "corrupted or if they for any reason were not able to solve the task. These are fields\n",
    "'cant_solve' and 'corrupt_data' given in the task_output.\n",
    "    a. How often does each occur in the project and do you see a trend within the\n",
    "        annotators that made use of these options?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'cant_solve' column\n",
    "cant_solve = results_data['cant_solve'].tolist()\n",
    "# Extract the 'corrupt_data' column\n",
    "corrupt_data = results_data['corrupt_data'].tolist()\n",
    "cant_solve_count=0\n",
    "cant_solve_annotator_index=[]\n",
    "# Calculate how often annotators answer with true for the 'cant_solve' field\n",
    "for index,answer in enumerate(cant_solve):\n",
    "    if str(answer) == 'True':\n",
    "        cant_solve_count=cant_solve_count+1\n",
    "        cant_solve_annotator_index.append(index)\n",
    "corrupt_data_count=0\n",
    "# Calculate how often annotators answer with true for the 'corrupt_data' field\n",
    "corrupt_data_annotator_index=[]\n",
    "for index,answer in enumerate(corrupt_data):\n",
    "    if str(answer) == 'True':\n",
    "        corrupt_data_count=corrupt_data_count+1\n",
    "        corrupt_data_annotator_index.append(index)\n",
    "# Determine the ID of annotators which used cant_solve field\n",
    "cant_solve_annotators = [vendor_user_id[x] for x in cant_solve_annotator_index]\n",
    "# Determine the ID of annotators which used corrupt_data field\n",
    "corrupt_data_annotators = [vendor_user_id[x] for x in corrupt_data_annotator_index]\n",
    "AmountOfUsageByAnnotators=annotator_count(cant_solve_annotators+corrupt_data_annotators)\n",
    "CheckForTrend=[index+1 for index,x in enumerate(AmountOfUsageByAnnotators) if x>=3]\n",
    "print(\"The 'cant_solve' and 'corrupt_data' fields occurred {0}, {1} times respectively\".format(cant_solve_count, corrupt_data_count))\n",
    "print('IDs of annotators that used cant_solve and corrupt_data fields more than or equal three times:'+str(sorted(CheckForTrend))+'\\n')\n",
    "print('Most likely there is a trend between those annotators to use cant_solve and corrupt_data fields'+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Is the reference set balanced? Please demonstrate via numbers and visualizations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_emotion_dist(dist, color_code='#C2185B', title=\"Plot\"):\n",
    "    \n",
    "    tmp_df = pd.DataFrame()\n",
    "    tmp_df['is_bicycle'] = list(dist.keys())\n",
    "    tmp_df['Count'] = list(dist)\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax = sns.barplot(x=\"is_bicycle\", y='Count', color=color_code, data=tmp_df)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "    plt.savefig('reference_set_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_data = pd.read_csv('references_dataset.csv')\n",
    "Label_distribution = ref_data.Label.value_counts()\n",
    "print(\"number of images which answered by True = \" +str(Label_distribution[1]))\n",
    "print(\"number of images which answ ered by False = \" +str(Label_distribution[0]))\n",
    "print(\"The reference dataset is balanced as it has approximately equal number of examples for both True and False answer as shown in the figure.\")\n",
    "plot_emotion_dist(Label_distribution, \"#2962FF\", \"Label distribution in reference dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. Using the reference set, can you identify good and bad annotators? Please use\n",
    "statistics and visualizations. Feel free to get creative.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the images labels in the references dataset\n",
    "Image_Classification =ref_data['Label']\n",
    "# Replace 'True' with 'yes' and 'False' with 'no' \n",
    "# to be convenient for the comparison with the annotators' answers\n",
    "Image_Classification =Image_Classification.replace( {True: 'yes', False: 'no'})\n",
    "Image_Classification= np.array(Image_Classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change the'vendor_user_id' column into numpy array\n",
    "annotators = np.array(vendor_user_id)\n",
    "shape = ( InputNodesLenght, InputNodeQuestions )\n",
    "# Shape the annotator's answers list into a matrix \n",
    "# to determine the annotators which participate in each question separately\n",
    "annotators_matrix =annotators.reshape( shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_scores_matrix = np.zeros((answer_matrix.shape[0],answer_matrix.shape[1]))\n",
    "# Scan the reference dataset and compare the reference answer of each question \n",
    "# with the annotarors' answers which participate in the annotation of the same image\n",
    "# then give a score '1' for the annotator right answer and '0' for the wrong answer\n",
    "for rows in range(0, answer_matrix.shape[0]):\n",
    "    for columns in range(0, answer_matrix.shape[1]):\n",
    "        if answer_matrix[rows][columns] ==Image_Classification[rows]:\n",
    "            annotator_scores_matrix[rows][columns]=1\n",
    "        else:\n",
    "            annotator_scores_matrix[rows][columns]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_score_list = np.zeros((22))\n",
    "# As each annotator contributes to different questions in the dataset,\n",
    "# the score of each annotator would be calculated by summing his\\her \n",
    "# score in all contributions. So, the efficiency of each annotator \n",
    "# can be determined with the ratio of the annotator score over his\\her number of contributions.\n",
    "for rows in range(0, annotators_matrix.shape[0]):\n",
    "    for columns in range(0, annotators_matrix.shape[1]):\n",
    "        if annotators_matrix[rows][columns] == 'annotator_01':\n",
    "            annotator_score_list[0]=annotator_score_list[0]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_02':\n",
    "            annotator_score_list[1]=annotator_score_list[1]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_03':\n",
    "            annotator_score_list[2]=annotator_score_list[2]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_04':\n",
    "            annotator_score_list[3]=annotator_score_list[3]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_05':\n",
    "            annotator_score_list[4]=annotator_score_list[4]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_06':\n",
    "            annotator_score_list[5]=annotator_score_list[5]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_07':\n",
    "            annotator_score_list[6]=annotator_score_list[6]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_08':\n",
    "            annotator_score_list[7]=annotator_score_list[7]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_09':\n",
    "            annotator_score_list[8]=annotator_score_list[8]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_10':\n",
    "            annotator_score_list[9]=annotator_score_list[9]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_11':\n",
    "            annotator_score_list[10]=annotator_score_list[10]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_12':\n",
    "            annotator_score_list[11]=annotator_score_list[11]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_13':\n",
    "            annotator_score_list[12]=annotator_score_list[12]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_14':\n",
    "            annotator_score_list[13]=annotator_score_list[13]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_15':\n",
    "            annotator_score_list[14]=annotator_score_list[14]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_16':\n",
    "            annotator_score_list[15]=annotator_score_list[15]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_17':\n",
    "            annotator_score_list[16]=annotator_score_list[16]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_18':\n",
    "            annotator_score_list[17]=annotator_score_list[17]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_19':\n",
    "            annotator_score_list[18]=annotator_score_list[18]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_20':\n",
    "            annotator_score_list[19]=annotator_score_list[19]+annotator_scores_matrix[rows][columns]\n",
    "        elif annotators_matrix[rows][columns] == 'annotator_21':\n",
    "            annotator_score_list[20]=annotator_score_list[20]+annotator_scores_matrix[rows][columns]\n",
    "        else:\n",
    "            annotator_score_list[21]=annotator_score_list[21]+annotator_scores_matrix[rows][columns]\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotator_efficiency = annotator_score_list/annotator_contribution_list\n",
    "print(\"The annotators' efficiency scores are: \" + str(annotator_efficiency))\n",
    "print(\"Note: Best score = 1 and Worst score = 0\")\n",
    "# Determine the threshold through calculating the average of the annotator efficiency\n",
    "Threshold = sum(annotator_efficiency)/len(annotator_efficiency)\n",
    "Threshold_line = np.ones(22)*Threshold\n",
    "plt.plot(annotator_efficiency,'.')\n",
    "plt.plot(Threshold_line)\n",
    "plt.title('Annotators score with respect to there contribution')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('annotators')\n",
    "plt.legend(['annotators cost function', 'Threshold line'], loc='lower right')\n",
    "plt.savefig('annotators_score_analysis.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_annotator=[]\n",
    "bad_annotator =[]\n",
    "# Determine the index of the annotators which have scores above average\n",
    "for index, efficiency in enumerate(annotator_efficiency):\n",
    "    if efficiency >= Threshold:\n",
    "        good_annotator.append(index+1)\n",
    "    else:\n",
    "        bad_annotator.append(index+1)\n",
    "\n",
    "good_annotator_list=['annotator_'+str(s) for s in good_annotator]\n",
    "bad_annotator_list=['annotator_'+str(s) for s in bad_annotator]\n",
    "print (\"The good annotator list: \" + str(good_annotator_list)+\"\\n\")\n",
    "print(\"The bad annotator list: \"+ str(bad_annotator_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
